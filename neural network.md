
深度前馈网络（deep feedforward network），也称作**前馈神经网络（feedforward neural network）**或者**多层感知机（multilayer perceptorn, MLP）**是典型的深度学习模型。

模型可以表示为一个有向无环图，假设有三个函数，则可以表示为：

$$
f(x) = f^{(3)}(f^{(2)}(f^{(3)}(x)))
$$

这种链式结构是神经网络中最常用的结构。在这种情况下，$f^{(1)}$ 被称为网络的第一层，$f^{(2)}$ 被称为第二层，最后一层 $f^{(3)}$ 则被称为**输出层（output layer）**。链的全长称为模型的**深度（depth）**。除了输出层的这些层也可以被称为**隐藏层（hidden layer）**。

**修正线性单元（ReLU）**


# 我的理解
神经网络的一个优势，取代特征工程。在传统机器学习的模型训练中，有一个被称为特征工程的环节，这个环节主要是期望能够挖掘出不同特征的组合，或者组合的组合对最终结果的影响。而神经网络会自己计算这个过程，因此就不需要进行特征工程了。


# 神经网络和深度学习

## 2.1. 二分类问题

以图片分类为例，定义二分类问题的输入输出和数据表示。图片可以表示为三个通道的颜色数值，一个通道就是一层与图片像素等大小的矩阵，一张图片有三个矩阵构成。

比如一张 $3 \times 2$ 的图片，可以表示为：
**R**
$$
\begin{bmatrix}
123 & 21  \\  
3 & 87  \\  
87 & 21 
\end{bmatrix}
$$

**G**
$$
\begin{bmatrix}
3 & 87 \\  
35 & 25 \\  
123 & 21
\end{bmatrix}
$$

**B**
$$
\begin{bmatrix}
31 & 55 \\  
76 & 11 \\  
31 & 21
\end{bmatrix}
$$

其表示为输入时，将三个举证依次展开成为一个 $1 \times n$ 的矩阵：
$$
\begin{bmatrix}
123 & 21 & \dots & 87 & 21 
\end{bmatrix}
$$

### 特征矩阵
**n**：一般用 n 表示一个特征中特征的长度

**m**：用 m 表示一个数据集中样本的数量

一般用矩阵的一列表示一个样本。

## 2.2. 逻辑回归
介绍逻辑回归的基本原理，逻辑回归可以拆分为两个部分，相当于一个复合函数，一个是 $\theta w$，另外一个就是 Sigmoid 函数。

## 2.3. 逻辑回归中的损失函数
介绍逻辑回归中损失函数的定义。

## 2.4. 梯度下降
介绍梯度下降算法的基本原理。

## 2.5. 导数
导数相当于是斜率，在一条直线上，各处的导数都是相同的。导数可以表示为：
$$
\frac{d}{da}f(a)
$$

## 2.6. 更多导数
对于函数 $f(a) = a^2$ 来说，其导数为：$\frac{d}{da}f(a) = 2a$

|$f(x)$|$\frac{d}{da}f(a)$|
|:-:|:-:|
|$a^2$|$2a$|
|$a^3$|$3a^2$|
|$\log(a)$|$\frac{1}{a}$|

## 2.7. 计算图 

数学概念：链式法则
嵌套三层的复合函数，最里层的变量的变化，会逐步影响到外围函数
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTE1NDE4MzQ5ODIsNTM1NTAxOTc1XX0=
-->